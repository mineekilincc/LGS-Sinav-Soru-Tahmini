# A100 OOM Fix - Notebook Cell DeÄŸiÅŸiklikleri
# Copy-paste these into your Colab notebook

## ========================================
## CELL 1: Model Loading (Section 4.2)
## ========================================
# Bu cell'i tamamen deÄŸiÅŸtir:

# Model'i yÃ¼kle (8-bit quantization ile)
print("ğŸ“¥ Model yÃ¼kleniyor... (Bu 5-10 dakika sÃ¼rebilir)")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    load_in_8bit=True,          # 8-bit quantization
    device_map="auto",           # Otomatik device mapping
    torch_dtype=torch.float16    # FP16 precision
)

print("âœ… Model yÃ¼klendi!")
print(f"   Device: {model.device}")
print(f"   Dtype: {model.dtype}")


## ========================================
## CELL 2: LoRA Preparation (Section 5.1)
## ========================================
# Model'i LoRA iÃ§in hazÄ±rla - Bu cell'e 1 satÄ±r ekle:

# Model'i LoRA iÃ§in hazÄ±rla
model = prepare_model_for_kbit_training(model)

# âœ… BU SATIRI EKLE:
model.gradient_checkpointing_enable()

print("âœ… Model LoRA iÃ§in hazÄ±rlandÄ±!")
print("âœ… Gradient checkpointing aktif!")


## ========================================
## CELL 3: Training Arguments (Section 7.1)
## ========================================
# Bu cell'i tamamen deÄŸiÅŸtir:

# Training arguments
# DÄ°KKAT: Bu parametreler A100 iÃ§in optimize edilmiÅŸtir!

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    
    # Training parametreleri - A100 OPTIMIZED
    num_train_epochs=3,
    per_device_train_batch_size=2,      # A100 iÃ§in optimize
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,       # Efektif batch size = 8
    
    # Optimizer parametreleri
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,                   # Ä°lk %3'te warmup
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # Precision
    bf16=True,                           # BF16 precision (A100 native)
    # fp16=True,                         # T4 GPU iÃ§in bu satÄ±rÄ± uncomment et
    
    # Memory optimization
    gradient_checkpointing=True,         # Memory tasarrufu
    
    # Logging ve kaydetme
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    save_total_limit=2,                  # Sadece son 2 checkpoint
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    
    # DiÄŸer
    report_to="none",                    # TensorBoard kapalÄ±
    seed=42,
    dataloader_num_workers=2,
    remove_unused_columns=False
)

print("âš™ï¸ A100 Training KonfigÃ¼rasyonu:")
print(f"  Epochs: {training_args.num_train_epochs}")
print(f"  Batch size: {training_args.per_device_train_batch_size}")
print(f"  Gradient accumulation: {training_args.gradient_accumulation_steps}")
print(f"  Efektif batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"  Learning rate: {training_args.learning_rate}")
print(f"  LR scheduler: {training_args.lr_scheduler_type}")
print(f"  Warmup ratio: {training_args.warmup_ratio}")
print(f"  Gradient checkpointing: âœ… ACTIVE")
print(f"\nâ±ï¸  Tahmini training sÃ¼resi: 45-60 dakika (A100)")


## ========================================
## Ã–ZET
## ========================================
"""
3 CELL DEÄÄ°ÅÄ°KLÄ°ÄÄ°:

1. Model Loading:
   - load_in_8bit=True (geri getir)
   - torch.float16 (geri getir)

2. LoRA Preparation:
   - model.gradient_checkpointing_enable() (ekle)

3. Training Arguments:
   - per_device_train_batch_size=2 (4â†’2)
   - per_device_eval_batch_size=2 (4â†’2)
   - gradient_accumulation_steps=4 (2â†’4)
   - gradient_checkpointing=True (ekle)
   - dataloader_num_workers=2 (4â†’2)

SONUÃ‡:
âœ… OOM hatasÄ± Ã§Ã¶zÃ¼lecek
âœ… Training sÃ¼resi: ~45-60 dakika
âœ… Hala T4'ten 2.5x hÄ±zlÄ±!
"""
