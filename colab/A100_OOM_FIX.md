# A100 Memory Fix - OOM HatasÄ± Ã‡Ã¶zÃ¼mÃ¼

## ğŸš¨ Problem
A100'de full precision + batch_size=4 = 40GB memory yetersiz!

## âœ… Ã‡Ã¶zÃ¼m

### ZORUNLU DeÄŸiÅŸiklikler:

#### 1. Model Loading'e Geri DÃ¶n (Section 4)
```python
# YANLIÅ (OOM veriyor):
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    load_in_8bit=False,  # âŒ Bu hata veriyor
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# DOÄRU (Ã‡alÄ±ÅŸÄ±yor):
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    load_in_8bit=True,   # âœ… 8-bit geri getir
    device_map="auto",
    torch_dtype=torch.float16  # âœ… FP16 yeter
)
```

#### 2. Training Arguments - A100 iÃ§in Optimize (Section 7)
```python
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    
    # Training parametreleri - A100 OPTIMIZED
    num_train_epochs=3,
    per_device_train_batch_size=2,      # âœ… 4â†’2 (OOM Ã¶nleme)
    per_device_eval_batch_size=2,       # âœ… 4â†’2
    gradient_accumulation_steps=4,       # âœ… 2â†’4 (efektif batch=8)
    
    # Optimizer parametreleri
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # Precision
    bf16=True,                          # âœ… A100 native BF16
    fp16=False,
    
    # Memory optimization
    gradient_checkpointing=True,        # âœ… EKLE! Memory tasarrufu
    
    # Logging ve kaydetme
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    
    # DiÄŸer
    report_to="none",
    seed=42,
    dataloader_num_workers=2,           # âœ… 4â†’2 (memory iÃ§in)
    remove_unused_columns=False
)
```

#### 3. LoRA HazÄ±rlÄ±k - Gradient Checkpointing Ekle (Section 5)
```python
# Model'i LoRA iÃ§in hazÄ±rla
model = prepare_model_for_kbit_training(model)

# âœ… EKLE: Gradient checkpointing aktif et
model.gradient_checkpointing_enable()

print("âœ… Model LoRA iÃ§in hazÄ±rlandÄ±!")
print("âœ… Gradient checkpointing aktif!")
```

## ğŸ“Š Yeni KonfigÃ¼rasyon

| Parametre | Ã–nceki | Yeni (OOM Fix) |
|-----------|--------|----------------|
| **load_in_8bit** | False | True âœ… |
| **torch_dtype** | bfloat16 | float16 âœ… |
| **batch_size** | 4 | 2 âœ… |
| **grad_accum** | 2 | 4 âœ… |
| **grad_checkpoint** | - | True âœ… |
| **workers** | 4 | 2 âœ… |

## â±ï¸ Yeni Tahmini SÃ¼re

- **T4 GPU**: ~2-3 saat
- **A100 GPU**: ~45-60 dakika (hala T4'ten 2.5-3x hÄ±zlÄ±!)

## ğŸ”§ Uygulama AdÄ±mlarÄ±

1. **Runtime'Ä± Restart Et**: Runtime â†’ Restart Runtime
2. **Section 4'Ã¼ DÃ¼zelt**: `load_in_8bit=True`, `torch.float16`
3. **Section 5'e Ekle**: `model.gradient_checkpointing_enable()`
4. **Section 7'yi DÃ¼zelt**: batch_size=2, grad_accum=4, grad_checkpoint=True
5. **Tekrar Ã‡alÄ±ÅŸtÄ±r**: Åimdi OOM olmayacak!

## ğŸ’¡ Neden Bu Ã‡alÄ±ÅŸÄ±yor?

- **8-bit quantization**: Model 14B params â†’ ~7GB (yarÄ± yarÄ±ya dÃ¼ÅŸer)
- **Gradient checkpointing**: Activation memory'yi trade-off eder (biraz yavaÅŸ ama Ã§ok az memory)
- **Batch size=2**: Her adÄ±mda daha az memory kullanÄ±r
- **Grad accum=4**: Efektif batch size hala 8 (aynÄ± quality)

**SonuÃ§**: OOM yok, hala hÄ±zlÄ±! ğŸš€
