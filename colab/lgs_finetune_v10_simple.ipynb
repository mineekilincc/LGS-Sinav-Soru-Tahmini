{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ¯ LGS Fine-Tune V10 - NÄ°HAÄ° + GPT OPTÄ°MÄ°ZE\n",
                "\n",
                "## âœ¨ TÃ¼m GPT Tavsiyeleri UygulandÄ±:\n",
                "- âœ… QLoRA r=32, alpha=64\n",
                "- âœ… lr=5e-5, epochs=3, warmup=0.03\n",
                "- âœ… packing=False, neftune kapalÄ±\n",
                "- âœ… **80-170 kelime** (tutarlÄ±)\n",
                "- âœ… **eval_steps=25** (daha sÄ±k deÄŸerlendirme)\n",
                "- âœ… **save_total_limit=2** (Drive ÅŸiÅŸmesin)\n",
                "- âœ… **Val split raporu** (daÄŸÄ±lÄ±m kontrolÃ¼)\n",
                "- âœ… **JSON Repair hÃ¼cresi** (parse hatalarÄ±nÄ± dÃ¼zelt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Kurulum\n",
                "print(\"â³ Kurulum...\")\n",
                "!pip install -q --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install -q unsloth_zoo\n",
                "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\n",
                "print(\"âœ… Kurulum tamamlandÄ±!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "PROJECT_PATH = \"/content/drive/MyDrive/lgs_soru_tahmin_projesi\"\n",
                "DATA_PATH = f\"{PROJECT_PATH}/data/lgs_finetune_data_v10_simple.jsonl\"\n",
                "MODEL_SAVE_PATH = f\"{PROJECT_PATH}/models/lgs_qwen_32b_v10\"\n",
                "\n",
                "import os\n",
                "if os.path.exists(DATA_PATH):\n",
                "    print(f\"âœ… Veri dosyasÄ± bulundu: {DATA_PATH}\")\n",
                "else:\n",
                "    print(f\"âŒ Veri dosyasÄ± bulunamadÄ±! YÃ¼kleyin: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Model YÃ¼kleme (4-bit QLoRA)\n",
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "print(\"â³ Model yÃ¼kleniyor (4-bit QLoRA)...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/Qwen2.5-32B-Instruct-bnb-4bit\",\n",
                "    max_seq_length = 4096,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "print(f\"âœ… Model yÃ¼klendi! VRAM: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. LoRA KonfigÃ¼rasyonu\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 32,\n",
                "    target_modules = [\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    lora_alpha = 64,\n",
                "    lora_dropout = 0.05,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    use_rslora = True,\n",
                ")\n",
                "print(\"âœ… LoRA: r=32, alpha=64, dropout=0.05\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. SERT System Prompt (80-170 kelime - TUTARLI)\n",
                "SYSTEM_PROMPT = \"\"\"Sen MEB LGS TÃ¼rkÃ§e soru yazarÄ±sÄ±n.\n",
                "\n",
                "KESÄ°N KURALLAR:\n",
                "1. SADECE JSON dÃ¶ndÃ¼r - JSON dÄ±ÅŸÄ±nda TEK KARAKTER yazma!\n",
                "2. dogru_cevap SADECE A, B, C veya D olabilir\n",
                "3. TÃ¼m alanlar ZORUNLU: metin, soru, sik_a, sik_b, sik_c, sik_d, dogru_cevap\n",
                "4. ÅÄ±klar benzer uzunlukta ve aynÄ± biÃ§imde olsun\n",
                "5. Metin 80-170 kelime arasÄ±nda olsun\n",
                "6. Ã‡eldiriciler mantÄ±klÄ± ve benzer tÃ¼rde olsun\n",
                "7. NumaralanmÄ±ÅŸ cÃ¼mle formatÄ± KULLANMA\n",
                "\n",
                "JSON FORMATI:\n",
                "{\"metin\": \"...\", \"soru\": \"...\", \"sik_a\": \"...\", \"sik_b\": \"...\", \"sik_c\": \"...\", \"sik_d\": \"...\", \"dogru_cevap\": \"A\"}\"\"\"\n",
                "\n",
                "print(\"âœ… System Prompt: 80-170 kelime (tutarlÄ±)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Veri YÃ¼kleme + Val DaÄŸÄ±lÄ±m Raporu\n",
                "import json\n",
                "from datasets import Dataset\n",
                "from collections import Counter\n",
                "\n",
                "def format_example(item):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "        {\"role\": \"user\", \"content\": item[\"user\"]},\n",
                "        {\"role\": \"assistant\", \"content\": item[\"assistant\"]}\n",
                "    ]\n",
                "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "    return {\"text\": text}\n",
                "\n",
                "def extract_alt_konu(user_text):\n",
                "    \"\"\"User prompt'undan alt konu Ã§Ä±karÄ±r.\"\"\"\n",
                "    if \"Alt Konu:\" in user_text:\n",
                "        return user_text.split(\"Alt Konu:\")[1].split(\"\\n\")[0].strip()\n",
                "    return \"Bilinmiyor\"\n",
                "\n",
                "# Veriyi yÃ¼kle\n",
                "data = []\n",
                "bad_lines = 0\n",
                "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
                "    for line_num, line in enumerate(f, 1):\n",
                "        try:\n",
                "            item = json.loads(line.strip())\n",
                "            if item.get(\"user\") and item.get(\"assistant\"):\n",
                "                item[\"alt_konu\"] = extract_alt_konu(item[\"user\"])\n",
                "                data.append(item)\n",
                "            else:\n",
                "                bad_lines += 1\n",
                "        except Exception as e:\n",
                "            bad_lines += 1\n",
                "\n",
                "print(f\"âœ… {len(data)} Ã¶rnek yÃ¼klendi\")\n",
                "if bad_lines > 0:\n",
                "    print(f\"   âš  {bad_lines} satÄ±r atlandÄ±\")\n",
                "\n",
                "# Dataset oluÅŸtur\n",
                "dataset = Dataset.from_list(data)\n",
                "dataset = dataset.map(format_example)\n",
                "\n",
                "# Train/Val split\n",
                "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "train_dataset = split[\"train\"]\n",
                "val_dataset = split[\"test\"]\n",
                "\n",
                "print(f\"   Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
                "\n",
                "# VAL DAÄILIM RAPORU (GPT Ã¶nerisi)\n",
                "print(\"\\nğŸ“Š Val Set Alt Konu DaÄŸÄ±lÄ±mÄ±:\")\n",
                "val_alt_konular = [data[i][\"alt_konu\"] for i in split[\"test\"].indices]\n",
                "for ak, count in Counter(val_alt_konular).most_common():\n",
                "    print(f\"   {ak}: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. EÄŸitim (GPT OPTÄ°MÄ°ZE AYARLAR)\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = train_dataset,\n",
                "    eval_dataset = val_dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = 4096,\n",
                "    packing = False,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 1,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_ratio = 0.03,\n",
                "        num_train_epochs = 3,\n",
                "        learning_rate = 5e-5,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 10,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"cosine\",\n",
                "        seed = 42,\n",
                "        output_dir = \"outputs\",\n",
                "        evaluation_strategy = \"steps\",\n",
                "        eval_steps = 25,  # GPT: Daha sÄ±k eval\n",
                "        save_strategy = \"steps\",\n",
                "        save_steps = 50,  # GPT: Daha sÄ±k kaydet\n",
                "        save_total_limit = 2,  # GPT: Drive ÅŸiÅŸmesin\n",
                "        load_best_model_at_end = True,\n",
                "        metric_for_best_model = \"eval_loss\",\n",
                "        greater_is_better = False,\n",
                "        report_to = \"none\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"ğŸš€ EÄŸitim baÅŸlatÄ±lÄ±yor...\")\n",
                "print(\"   eval_steps=25, save_steps=50, save_total_limit=2\")\n",
                "trainer.train()\n",
                "print(\"âœ… EÄŸitim tamamlandÄ±!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Kalite DeÄŸerlendirmesi (80-170 kelime - TUTARLI)\n",
                "import re\n",
                "\n",
                "def evaluate_output(response):\n",
                "    metrics = {\n",
                "        \"json_valid\": False,\n",
                "        \"all_fields\": False,\n",
                "        \"dogru_cevap_valid\": False,\n",
                "        \"no_duplicate_options\": False,\n",
                "        \"metin_length_ok\": False,\n",
                "        \"options_balanced\": False,\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        start = response.find('{')\n",
                "        end = response.rfind('}') + 1\n",
                "        if start < 0 or end <= start:\n",
                "            return metrics, 0\n",
                "        \n",
                "        data = json.loads(response[start:end])\n",
                "        metrics[\"json_valid\"] = True\n",
                "        \n",
                "        required = [\"metin\", \"soru\", \"sik_a\", \"sik_b\", \"sik_c\", \"sik_d\", \"dogru_cevap\"]\n",
                "        metrics[\"all_fields\"] = all(data.get(f) for f in required)\n",
                "        \n",
                "        dc = data.get(\"dogru_cevap\", \"\").upper()\n",
                "        metrics[\"dogru_cevap_valid\"] = dc in [\"A\", \"B\", \"C\", \"D\"]\n",
                "        \n",
                "        options = [data.get(f, \"\") for f in [\"sik_a\", \"sik_b\", \"sik_c\", \"sik_d\"]]\n",
                "        metrics[\"no_duplicate_options\"] = len(set(options)) == 4 and all(options)\n",
                "        \n",
                "        # 80-170 kelime (System prompt ile TUTARLI)\n",
                "        metin = data.get(\"metin\", \"\")\n",
                "        word_count = len(metin.split())\n",
                "        metrics[\"metin_length_ok\"] = 70 <= word_count <= 180  # Biraz esnek\n",
                "        \n",
                "        opt_lens = [len(o) for o in options if o]\n",
                "        if len(opt_lens) == 4 and min(opt_lens) > 0:\n",
                "            ratio = max(opt_lens) / min(opt_lens)\n",
                "            metrics[\"options_balanced\"] = ratio < 3.0\n",
                "        \n",
                "    except:\n",
                "        pass\n",
                "    \n",
                "    score = sum(metrics.values()) / len(metrics) * 100\n",
                "    return metrics, score\n",
                "\n",
                "print(\"âœ… Kalite deÄŸerlendirme: 80-170 kelime (tutarlÄ±)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. JSON Repair Fonksiyonu (GPT Ã–NERÄ°SÄ°)\n",
                "def repair_json(model, tokenizer, broken_output):\n",
                "    \"\"\"Bozuk JSON'u dÃ¼zeltmeye Ã§alÄ±ÅŸÄ±r.\"\"\"\n",
                "    repair_prompt = f\"\"\"AÅŸaÄŸÄ±daki metni sadece geÃ§erli JSON'a dÃ¶nÃ¼ÅŸtÃ¼r.\n",
                "BaÅŸka hiÃ§bir ÅŸey yazma, sadece dÃ¼zeltilmiÅŸ JSON dÃ¶ndÃ¼r.\n",
                "\n",
                "BOZUK:\n",
                "{broken_output[:1000]}\n",
                "\n",
                "DÃœZELTÄ°LMÄ°Å JSON:\"\"\"\n",
                "    \n",
                "    messages = [{\"role\": \"user\", \"content\": repair_prompt}]\n",
                "    \n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=True,\n",
                "        add_generation_prompt=True,\n",
                "        return_tensors=\"pt\"\n",
                "    ).to(\"cuda\")\n",
                "    \n",
                "    input_length = inputs.shape[-1]\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        inputs,\n",
                "        max_new_tokens=512,\n",
                "        temperature=0.3,\n",
                "        do_sample=True,\n",
                "        pad_token_id=tokenizer.eos_token_id,\n",
                "    )\n",
                "    \n",
                "    new_tokens = outputs[0][input_length:]\n",
                "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
                "\n",
                "print(\"âœ… JSON Repair fonksiyonu hazÄ±r\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. Test (JSON Repair dahil)\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "test_prompts = [\n",
                "    \"Konu: Paragraf\\nAlt Konu: Ana DÃ¼ÅŸÃ¼nce\\n\\nBu kriterlere gÃ¶re LGS TÃ¼rkÃ§e sorusu Ã¼ret.\",\n",
                "    \"Konu: CÃ¼mlede Anlam\\nAlt Konu: Sebep-SonuÃ§\\n\\nBu kriterlere gÃ¶re LGS TÃ¼rkÃ§e sorusu Ã¼ret.\",\n",
                "    \"Konu: Dil Bilgisi\\nAlt Konu: Fiilimsiler\\n\\nBu kriterlere gÃ¶re LGS TÃ¼rkÃ§e sorusu Ã¼ret.\",\n",
                "    \"Konu: SÃ¶zcÃ¼kte Anlam\\nAlt Konu: Ã‡ok AnlamlÄ±lÄ±k\\n\\nBu kriterlere gÃ¶re LGS TÃ¼rkÃ§e sorusu Ã¼ret.\",\n",
                "    \"Konu: CÃ¼mlede Anlam\\nAlt Konu: Deyim\\n\\nBu kriterlere gÃ¶re LGS TÃ¼rkÃ§e sorusu Ã¼ret.\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ§ª Test baÅŸlatÄ±lÄ±yor (JSON Repair dahil)...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "total_score = 0\n",
                "repair_count = 0\n",
                "\n",
                "for i, prompt in enumerate(test_prompts, 1):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "    \n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=True,\n",
                "        add_generation_prompt=True,\n",
                "        return_tensors=\"pt\"\n",
                "    ).to(\"cuda\")\n",
                "    \n",
                "    input_length = inputs.shape[-1]\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        inputs,\n",
                "        max_new_tokens=1024,\n",
                "        temperature=0.7,\n",
                "        do_sample=True,\n",
                "        top_p=0.9,\n",
                "        pad_token_id=tokenizer.eos_token_id,\n",
                "    )\n",
                "    \n",
                "    new_tokens = outputs[0][input_length:]\n",
                "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
                "    \n",
                "    metrics, score = evaluate_output(response)\n",
                "    \n",
                "    # JSON parse baÅŸarÄ±sÄ±z ise repair dene\n",
                "    if not metrics[\"json_valid\"]:\n",
                "        print(f\"   ğŸ”§ JSON repair deneniyor...\")\n",
                "        repaired = repair_json(model, tokenizer, response)\n",
                "        metrics, score = evaluate_output(repaired)\n",
                "        if metrics[\"json_valid\"]:\n",
                "            repair_count += 1\n",
                "            print(f\"   âœ… Repair baÅŸarÄ±lÄ±!\")\n",
                "    \n",
                "    total_score += score\n",
                "    \n",
                "    alt_konu = prompt.split(\"Alt Konu:\")[1].split(\"\\n\")[0].strip()\n",
                "    print(f\"\\nTest {i}: {alt_konu}\")\n",
                "    print(f\"   Skor: {score:.0f}/100\")\n",
                "    for k, v in metrics.items():\n",
                "        status = \"âœ…\" if v else \"âŒ\"\n",
                "        print(f\"   {status} {k}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "avg_score = total_score / len(test_prompts)\n",
                "print(f\"ğŸ“Š ORTALAMA SKOR: {avg_score:.0f}/100\")\n",
                "print(f\"ğŸ”§ JSON Repair kullanÄ±ldÄ±: {repair_count}/{len(test_prompts)}\")\n",
                "\n",
                "if avg_score >= 80:\n",
                "    print(\"ğŸ‰ Model baÅŸarÄ±lÄ±!\")\n",
                "elif avg_score >= 60:\n",
                "    print(\"âš ï¸ Model iyi ama iyileÅŸtirme gerekebilir\")\n",
                "else:\n",
                "    print(\"âŒ Model yetersiz\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 11. Model Kaydetme\n",
                "print(\"ğŸ’¾ Model kaydediliyor...\")\n",
                "model.save_pretrained(MODEL_SAVE_PATH)\n",
                "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
                "print(f\"âœ… Model kaydedildi: {MODEL_SAVE_PATH}\")\n",
                "print(\"\\nğŸ“‹ Sonraki adÄ±mlar:\")\n",
                "print(\"   1. run_model_api_v10.ipynb Ã§alÄ±ÅŸtÄ±rÄ±n\")\n",
                "print(\"   2. URL'yi .env dosyasÄ±na ekleyin\")\n",
                "print(\"   3. python src/web_app_v3.py baÅŸlatÄ±n\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}