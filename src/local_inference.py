# -*- coding: utf-8 -*-
"""
LGS Fine-Tuned Model - Yerel Inference
======================================
EÄŸitilmiÅŸ Llama-3 modelini yÃ¼kleyip soru Ã¼retir.
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from typing import Dict, Any, Optional

class LocalLGSModel:
    """Fine-tuned Llama-3 modelini yÃ¼kler ve inference yapar."""
    
    def __init__(
        self,
        adapter_path: str = "models/lgs_turkish_lora",
        base_model: str = "unsloth/llama-3-8b-Instruct-bnb-4bit",
        device: str = "auto"
    ):
        self.adapter_path = adapter_path
        self.base_model = base_model
        self.device = device
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """Modeli ve tokenizer'Ä± yÃ¼kler."""
        print(f"ğŸ”„ Base model yÃ¼kleniyor: {self.base_model}")
        
        # GPU kontrolÃ¼
        has_cuda = torch.cuda.is_available()
        if has_cuda:
            print(f"âœ… CUDA GPU bulundu: {torch.cuda.get_device_name(0)}")
        else:
            print("âš ï¸ GPU bulunamadÄ± - CPU modunda Ã§alÄ±ÅŸÄ±lacak (yavaÅŸ olabilir)")
        
        # 4-bit quantization config (sadece GPU varsa)
        if has_cuda:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
            )
            device_map_arg = self.device
        else:
            # CPU modunda quantization kullanma
            bnb_config = None
            device_map_arg = "cpu"
        
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model,
            trust_remote_code=True
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        
        # Base model
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                quantization_config=bnb_config,
                device_map=device_map_arg,
                trust_remote_code=True,
                torch_dtype=torch.float16 if has_cuda else torch.float32
            )
        except Exception as e:
            print(f"âš ï¸ Quantized model yÃ¼klenemedi: {e}")
            print("ğŸ”„ Fallback: Normal model yÃ¼kleniyor...")
            # Fallback: quantization olmadan yÃ¼kle
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model.replace("-bnb-4bit", ""),  # Quantized olmayan versiyonu dene
                device_map="cpu",
                trust_remote_code=True,
                torch_dtype=torch.float32
            )
        
        print(f"âœ… Base model yÃ¼klendi")
        
        # Adapter'Ä± yÃ¼kle
        if os.path.exists(self.adapter_path):
            print(f"ğŸ”„ Adapter yÃ¼kleniyor: {self.adapter_path}")
            self.model = PeftModel.from_pretrained(
                self.model,
                self.adapter_path,
                is_trainable=False
            )
            print("âœ… Fine-tuned adapter yÃ¼klendi")
        else:
            print(f"âš ï¸ Adapter bulunamadÄ±: {self.adapter_path}")
            print("Base model kullanÄ±lacak (fine-tune olmadan)")
        
        self.model.eval()
        print("âœ… Model inference modunda hazÄ±r")
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 1024,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> str:
        """Prompt'tan metin Ã¼retir."""
        
        if self.model is None:
            raise RuntimeError("Model henÃ¼z yÃ¼klenmedi. Ã–nce load_model() Ã§aÄŸÄ±rÄ±n.")
        
        # Llama-3 Instruct formatÄ±
        formatted_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Sen MEB LGS TÃ¼rkÃ§e soru yazma konusunda uzmanlaÅŸmÄ±ÅŸ bir yapay zeka asistanÄ±sÄ±n.<|eot_id|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        # Tokenize
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()


# Global instance (lazy loading)
_model_instance: Optional[LocalLGSModel] = None

def get_model() -> LocalLGSModel:
    """Singleton pattern ile model instance dÃ¶ndÃ¼rÃ¼r."""
    global _model_instance
    if _model_instance is None:
        _model_instance = LocalLGSModel()
        _model_instance.load_model()
    return _model_instance


if __name__ == "__main__":
    # Test
    print("=" * 60)
    print("YEREL MODEL TEST")
    print("=" * 60)
    
    model = get_model()
    
    test_prompt = """Paragraf konusunda, Ana DÃ¼ÅŸÃ¼nce alt konusunda, orta zorlukta bir LGS sorusu Ã¼ret.

Metin: Bilim ve teknoloji hakkÄ±nda 50-60 kelimelik bir paragraf yaz.

Soru: Bu metinde anlatÄ±lmak istenen aÅŸaÄŸÄ±dakilerden hangisidir?

A) [SeÃ§enek]
B) [SeÃ§enek]
C) [SeÃ§enek]
D) [SeÃ§enek]

DoÄŸru Cevap: [A/B/C/D]"""
    
    print("\nğŸ“ Test Promptu:")
    print(test_prompt[:200] + "...")
    
    print("\nğŸ¤– Model Ã‡Ä±ktÄ±sÄ±:")
    result = model.generate(test_prompt, max_new_tokens=512)
    print(result)
