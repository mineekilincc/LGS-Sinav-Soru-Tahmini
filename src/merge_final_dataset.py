# -*- coding: utf-8 -*-
"""
VERÄ° BÄ°RLEÅTÄ°RME VE FINAL DATASET
==================================
GPT sorularÄ± + v12_quality_filtered â†’ v13_balanced_final
"""

import json
import re
from pathlib import Path
from collections import Counter

def count_words(text):
    """Kelime sayÄ±sÄ±nÄ± hesapla."""
    return len(re.findall(r'\b\w+\b', text or "", re.UNICODE))

def convert_gpt_to_jsonl(gpt_path, output_path):
    """GPT formatÄ±ndan bizim JSONL formatÄ±mÄ±za Ã§evir."""
    
    # GPT JSON'unu yÃ¼kle
    with open(gpt_path, 'r', encoding='utf-8') as f:
        gpt_data = json.load(f)
    
    print(f"ğŸ“Š GPT sorularÄ± yÃ¼klendi: {len(gpt_data)}")
    
    converted = []
    stats = {"total": len(gpt_data), "success": 0, "failed": 0}
    word_counts = []
    
    for item in gpt_data:
        try:
            konu = item.get("konu", "")
            alt_konu = item.get("alt_konu", "")
            
            # Assistant JSON oluÅŸtur
            assistant_obj = {
                "metin": item.get("metin", ""),
                "soru": item.get("soru", ""),
                "sik_a": item.get("sik_a", ""),
                "sik_b": item.get("sik_b", ""),
                "sik_c": item.get("sik_c", ""),
                "sik_d": item.get("sik_d", ""),
                "dogru_cevap": item.get("dogru_cevap", "")
            }
            
            # Kelime sayÄ±sÄ±
            wc = count_words(assistant_obj["metin"])
            word_counts.append(wc)
            
            # JSONL formatÄ±
            converted_item = {
                "user": f"Konu: {konu}\nAlt Konu: {alt_konu}\n\nBu kriterlere gÃ¶re LGS TÃ¼rkÃ§e sorusu Ã¼ret.",
                "assistant": json.dumps(assistant_obj, ensure_ascii=False)
            }
            
            converted.append(converted_item)
            stats["success"] += 1
            
        except Exception as e:
            print(f"   âŒ Hata: {str(e)}")
            stats["failed"] += 1
            continue
    
    # Kaydet
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\\n')
    
    print(f"\\nâœ… DÃ¶nÃ¼ÅŸtÃ¼rme tamamlandÄ±:")
    print(f"   BaÅŸarÄ±lÄ±: {stats['success']}")
    print(f"   BaÅŸarÄ±sÄ±z: {stats['failed']}")
    print(f"   Kelime sayÄ±sÄ± (ort): {sum(word_counts)/len(word_counts):.1f}")
    print(f"   Kaydedildi: {output_path}")
    
    return stats

def merge_datasets(v12_path, gpt_path, output_dir):
    """v12 + GPT â†’ v13_balanced_final"""
    
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # v12 yÃ¼kle
    v12_data = []
    with open(v12_path, 'r', encoding='utf-8') as f:
        for line in f:
            v12_data.append(json.loads(line.strip()))
    
    # GPT yÃ¼kle
    gpt_data = []
    with open(gpt_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                gpt_data.append(json.loads(line))
            except Exception as e:
                print(f"   âš ï¸ SatÄ±r parse edilemedi: {str(e)[:50]}")
                continue
    
    print(f"\\n{'='*70}")
    print(f"VERÄ° BÄ°RLEÅTÄ°RME")
    print(f"{'='*70}")
    print(f"v12_quality_filtered: {len(v12_data)}")
    print(f"GPT generated:        {len(gpt_data)}")
    print(f"Toplam:               {len(v12_data) + len(gpt_data)}")
    
    # BirleÅŸtir
    all_data = v12_data + gpt_data
    
    # Train/Val split (90/10)
    import random
    random.seed(42)
    random.shuffle(all_data)
    
    split_idx = int(len(all_data) * 0.9)
    train_data = all_data[:split_idx]
    val_data = all_data[split_idx:]
    
    # Kaydet
    train_path = output_dir / "train.jsonl"
    val_path = output_dir / "val.jsonl"
    
    with open(train_path, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\\n')
    
    with open(val_path, 'w', encoding='utf-8') as f:
        for item in val_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\\n')
    
    print(f"\\nâœ… BirleÅŸtirme tamamlandÄ±:")
    print(f"   Train: {len(train_data)} ({train_path})")
    print(f"   Val:   {len(val_data)} ({val_path})")
    
    # DaÄŸÄ±lÄ±m analizi
    train_dist = Counter()
    for item in train_data:
        user = item.get("user", "")
        if "Konu:" in user and "Alt Konu:" in user:
            konu = user.split("Konu:")[1].split("\n")[0].strip()
            alt_konu = user.split("Alt Konu:")[1].split("\n")[0].strip()
            train_dist[f"{konu}_{alt_konu}"] += 1
    
    print(f"\\n{'='*70}")
    print(f"TRAIN DAÄILIMI:")
    print(f"{'='*70}")
    for key, count in sorted(train_dist.items()):
        print(f"   {key:50s}: {count:4d}")
    
    # Kelime sayÄ±sÄ± analizi
    word_counts = []
    for item in train_data:
        try:
            assistant_obj = json.loads(item.get("assistant", "{}"))
            metin = assistant_obj.get("metin", "")
            wc = count_words(metin)
            word_counts.append(wc)
        except:
            pass
    
    if word_counts:
        print(f"\\n{'='*70}")
        print(f"KELÄ°ME SAYISI (TRAIN):")
        print(f"{'='*70}")
        print(f"   Ortalama: {sum(word_counts)/len(word_counts):.1f}")
        print(f"   Min:      {min(word_counts)}")
        print(f"   Max:      {max(word_counts)}")
        
        # DaÄŸÄ±lÄ±m
        ranges = {
            "<50": sum(1 for w in word_counts if w < 50),
            "50-80": sum(1 for w in word_counts if 50 <= w < 80),
            "80-120": sum(1 for w in word_counts if 80 <= w < 120),
            "120-150": sum(1 for w in word_counts if 120 <= w <= 150),
            "150-180": sum(1 for w in word_counts if 150 < w <= 180),
            ">180": sum(1 for w in word_counts if w > 180),
        }
        
        print(f"\\n   DAÄILIM:")
        for range_name, count in ranges.items():
            pct = 100 * count / len(word_counts)
            print(f"      {range_name:10s}: {count:4d} ({pct:5.1f}%)")
    
    return {
        "train": len(train_data),
        "val": len(val_data),
        "total": len(all_data)
    }

if __name__ == "__main__":
    project_dir = Path(__file__).parent.parent
    
    # Paths
    gpt_json = project_dir / "data" / "questions.json"
    gpt_jsonl = project_dir / "data" / "temp" / "gpt_converted_fixed.jsonl"
    v12_train = project_dir / "data" / "v12_quality_filtered" / "train.jsonl"
    output_dir = project_dir / "data" / "v13_balanced_final"
    
    gpt_jsonl.parent.mkdir(exist_ok=True)
    
    print("="*70)
    print("FINAL DATASET OLUÅTURMA")
    print("="*70)
    
    # 1. GPT formatÄ±nÄ± dÃ¶nÃ¼ÅŸtÃ¼r
    print("\\n1ï¸âƒ£ GPT formatÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    convert_gpt_to_jsonl(gpt_json, gpt_jsonl)
    
    # 2. BirleÅŸtir
    print("\\n2ï¸âƒ£ Veri setleri birleÅŸtiriliyor...")
    result = merge_datasets(v12_train, gpt_jsonl, output_dir)
    
    print(f"\\n{'='*70}")
    print(f"âœ… FINAL DATASET HAZIR!")
    print(f"{'='*70}")
    print(f"KlasÃ¶r: {output_dir}")
    print(f"Train:  {result['train']} Ã¶rnek")
    print(f"Val:    {result['val']} Ã¶rnek")
    print(f"Toplam: {result['total']} Ã¶rnek")
